```{r setup50, include=FALSE}
library(tidyverse)
library(quantmod)
if (knitr::is_html_output()) {
  options(knitr.table.format = "html")
} else {
  options(knitr.table.format = "latex") 
}
```

```{r setup50-plots, include=FALSE}
if (knitr::is_html_output()) {
  knitr::opts_chunk$set(out.width = '100%') 
} else {
  knitr::opts_chunk$set(out.width = '100%')
}
```

```{r script2, code = readLines("scripts/Q2.R"), include=FALSE, cache=TRUE}
```

```{r dates, include=FALSE, cache=TRUE}
getSymbols(Symbols = "^GSPC",
           from    = '2007-01-03',
           to      = '2019-01-01',
           src     = 'yahoo')
```


# Exercise 2

In this exercise we examine two indices: S&P 500 (commonly known as S&P500, with ticker ^GSPC) which is a stock market price index that that measures the stock performance of 500 large companies listed on exchanges in the United States. Many institutional and private investors consider it to be one of the best representations of the U.S. stock market. Dow Jones Industrial Average (commonly known as DOW, with ticker ^DJI) which is a stock market price index that that measures the stock performance of 30 large companies listed on exchanges in the United States. Many consider the DOW to not be a good representation of the U.S. stock market and consider the S&P500, which also includes the 30 components of the DOW, to be a better representation of the U.S. stock market. We obtain both through the dataset given in the assignment, spanning from 2007-01-03 to 2019-01-01, thus the Financial Crisis is included.

## Theoretical Part

We consider the bivariate random vector at time $t> 0$
$$\bm{Y}_t = \begin{pmatrix} Y_{1,t}\\ Y_{2,t} \end{pmatrix},\quad\quad t=1,\dots,T,$$

where $Y_{1,t}$ and $Y_{2,t}$ are the given SP&500 and DOW returns. We assume that $Y_t$ is a bivariate zero mean Gaussian
$$\bm{Y}_t \mid \mathcal{F}_{t-1} \sim N(\bm{0}, \bm{\Sigma}_t).$$

We want to derive the Dynamic Conditional Correlations model for the above setup, which parameterize the conditional correlations directly. Using DCC models have a computational advantage over multivariate GARCH models, in the sense that the number of parameters to be estimated in the correlation process is independent of the number of series to be correlated.

Empirical findings by [@Engle2002] show that the DCC is often the most accurate when compared to a simple multivariate GARCH and several other estimators including the exponential moving average and 100-day moving average. His findings show that this is true whether the performance criterion is mean absolute error, diagnostic tests, or tests based on value at risk calculations.

To obtain the DCC model in our setup we rewrite the covariance matrix, $\bm{\Sigma}_t$, such that we obtain
$$\bm{Y}_t \mid \mathcal{F}_{t-1} \sim N(\bm{0}, \bm{D}_t^{\frac{1}{2}}\bm{R}_t \bm{D}_t^{\frac{1}{2}}),$$

here $\bm{D}_t$ is the diagonal matrix with elements
$$\sigma_{it}^2=\mathbb{V}(Y_{it}\mid \mathcal{F}_{t-1}),$$

which we assume follows a Generalized Autoregressive Conditional Heteroskedasticity process of order $p=q=1$ or more commonly known as the $\text{GARCH}(1,1)$ process for $i=1,2$. The updating equation for $\sigma_{it}^2$ thus becomes
$$\sigma_{it}^2=\omega + \alpha Y_{it-1} + \beta \sigma_{it-1}^2,$$

where $\omega,\alpha$ and $\beta$ are unknown coefficients. $\bm{R}_t$ is the correlation matrix with elements
$$\rho_{ijt}=\text{cor}(Y_{it}, Y_{jt}\mid \mathcal{F}_{t-1}).$$

Lastly the DCC model assumes that the correlation matrix, $\bm{R}_t$, can be decomposed into
$$\bm{R}_t = \tilde{\bm{Q}}_t^{-\frac{1}{2}}\bm{Q}_t\tilde{\bm{Q}}_t^{-\frac{1}{2}},$$

here $\tilde{\bm{Q}}_t$ is a diagonal matrix which contains the diagonal elements of the conditional covariance matrix, $\bm{Q}_t$, which is given as
$$\bm{Q}_t = \bar{\bm{Q}}(1-a-b)+a\bm{\eta}_{t-1}\bm{\eta}_{t-1}'+ b\bm{Q}_{t-1},$$

where $a$ and $b$ are unknown coefficients, $\bm{\eta}_t=\bm{D}_t^{-\frac{1}{2}}\bm{Y}_t$ thus the standardized residuals and $\bar{\bm{Q}}$ is fixed to to the empirical correlation of $\bm{\eta}_t$. To ensure that $\bm{Q}_t$ is positive definite we muse impose the constraint $a+b<1$, which will also guarantee that our model is mean reverting.

To obtain the log likelihood we use the Gaussianity of our returns to obtain
\begin{align*}
\ln(L_T)&=-\frac{1}{2}\sum_{t=1}^T (2\ln(2\pi)+\ln(|\bm{\Sigma}_t|)+\bm{Y}_t'\bm{\Sigma}_t^{-1}\bm{Y}_t)\\
        &=-\frac{1}{2}\sum_{t=1}^T (2\ln(2\pi)+\ln(|\bm{D}_t^{\frac{1}{2}}\bm{R}_t \bm{D}_t^{\frac{1}{2}}|)+\bm{Y}_t'\bm{D}_t^{-\frac{1}{2}}\bm{R}_t^{-1} \bm{D}_t^{-\frac{1}{2}}\bm{Y}_t)\\
        &=-\frac{1}{2}\sum_{t=1}^T (2\ln(2\pi)+\ln(|\bm{D}_t|)+\ln(|\bm{R}_t|)+\bm{\eta}_t' \bm{R}_{t}^{-1} \bm{\eta}_t)\\
        &=-\frac{1}{2}\sum_{t=1}^T (2\ln(2\pi)+\ln(|\bm{D}_t|)+\ln(|\bm{R}_t|)+\bm{\eta}_t' \bm{R}_{t}^{-1} \bm{\eta}_t + \bm{Y}_t' \bm{D}_t^{-\frac{1}{2}} \bm{D}_t^{-\frac{1}{2}} \bm{Y}_t - \bm{\eta}_t'\bm{\eta}_t)\\
        &=-\frac{1}{2}\sum_{t=1}^T (2\ln(2\pi)+\ln(|\bm{D}_t|) + \bm{Y}_t'\bm{D}_t^{-1}Y_t) - \frac{1}{2}\sum_{t=1}^T (\bm{\eta}_t' \bm{R}_{t}^{-1}\bm{\eta}_t -\bm{\eta}_t'\bm{\eta}_t+\ln(|\bm{R}_t|)).
\end{align*}

Where we use that $|\bm{A}\bm{B}\bm{C}|=|\bm{A}||\bm{B}||\bm{C}|$ and that $\ln(|\bm{A}\bm{B}|)=\ln(|\bm{A}||\bm{B}|)=\ln(|\bm{A}|)+\ln(|\bm{B}|)$.

Thus we have factorized the log likelihood into two parts: a volatility component and a correlation component. These are
\begin{align*}
L_{V}(\theta)\equiv \ln(L_{V,T}(\theta))&=-\frac{1}{2}\sum_{t=1}^T (2\ln(2\pi)+\ln(|\bm{D}_t|) + \bm{Y}_t'\bm{D}_t^{-1}\bm{Y}_t)\\
                                        &=-\frac{1}{2}\sum_{t=1}^T \sum_{i=1}^2 \left(\ln(2\pi)+\ln(\sigma_{it}^2)+\frac{r_{it}^2}{\sigma_{it}^2}\right),
\end{align*}

and 
$$L_{C}(\theta,\Phi)\equiv \ln(L_{C,T}(\theta,\Phi))=- \frac{1}{2}\sum_{t=1}^T (\bm{\eta}_t' \bm{R}_{t}^{-1}\bm{\eta}_t -\bm{\eta}_t'\bm{\eta}_t+\ln(|\bm{R}_t|)).$$

Here $\theta$ denotes the parameter vector in $\bm{D}_t$ and $\Phi$ denotes the parameter vector in $\bm{R}_t$. Thus
$$L(\theta,\Phi)=L_V(\theta)+L_C(\theta,\Phi).$$

Maximizing the log likelihood becomes a two-step approach. In the first step we find
$$\hat{\theta}=\arg\max\{L_V(\theta)\},$$

and then use this value as given in the next step
$$\max_{\phi}{L_C(\hat{\theta},\Phi)}.$$

Under reasonable regularity conditions [@Engle2002], consistency of the first step will ensure consistency of the second step.

We can derive the Constant Conditional Correlations model as a special case of the DCC model. DCC only differs in allowing $\bm{R}$ to be time dependent. Thus the CCC model assumes
$$\bm{Y}_t \mid \mathcal{F}_{t-1} \sim N(\bm{0}, \bm{D}_t^{\frac{1}{2}}\bm{R} \bm{D}_t^{\frac{1}{2}}),$$

where $\bm{R}$ is the correlation matrix containing the conditional correlations
$$\mathbb{E}(\bm{\eta}_{t}\bm{\eta}_t'\mid \mathcal{F}_{t-1})=\bm{D}_t^{-\frac{1}{2}}\bm{\Sigma}_t\bm{D}_t^{-\frac{1}{2}}.$$

To obtain the same $\bm{Q}_t$ throughout time we then set $a=b=0$, such that $\bm{Q}_t=\bm{Q}$.

## Computational Part

The relevant code can be found in Appendix file 'Code_Flow_38.R', attached with the document through WiseFlow. The code is fully documented with comments throughout all the functions.

To estimate the GARCH(1,1) model as discussed in the above Theoretical Part, we create four functions ***GARCHFilter***, ***ObjectiveFunction***, ***ineqfun_GARCH_WS*** and ***EstimateGARCH***.

***GARCHFilter*** is the filter for the GARCH(1,1) process. It takes a vector of values and the parameters as input. Here we take use of the updating equation for $\sigma_{it}^2$ established in the Theoretical Part. Through a loop we compute the next value of $\sigma_{it}^2$, from $t=1,\dots, T$, where we set the first variance to the empirical variance of the first $10 \%$ of the observations. Further we compute the log likelihood associated with the parameters and the values of $\sigma_{it}^2$, using the established function $L_{V}(\theta)$ as deduced in the Theoretical Part.

***ObjectiveFunction*** is the helper function for finding the maximum likelihood estimates of our GARCH(1,1) parameters. It takes the vector of values and a vector of parameters as input. It computes the negative log likelihood
$$N\mathcal{L}=-L_{V}(\theta).$$

***ineqfun_GARCH_WS*** serves as a basis to evaluate the inner part of the inequality constraints that need to be satisfied to impose weak stationarity, which is
$$0<\alpha+\beta<1.$$

***EstimateGARCH*** estimates the GARCH(1,1) model by first finding maximum likelihood estimates of our parameters, which are obtained by optimizing the negative log likelihood. The used optimizer, ***solnp***, is available through the **Rsolnp** R package, we set initial starting values, essentially we set starting value for $\alpha$ and $\beta$ and set $\omega$ to target the unconditional variance of the GARCH(1,1) model. After convergence of a solution the final parameters are then feeded to the ***GARCH_Filter*** to obtain the final filtered values of $\sigma_{it}^2$ and the log likelihood value. We also compute the Bayesian Information Criterion and the standardized residuals $\bm{\eta}_t$.

To estimate the DCC (CCC) model as discussed in the above Theoretical Part (and similiarly for the following models in the Empirical Part), we create two functions ***DCCFilter*** and ***Estimate_DCC***.

***DCCFilter*** is the filter for the DCC (CCC) model. It takes the vector of standardized residuals, the parameters and the unconditional correlation as input. Here we take use of the equations derived for $\bm{Q}_t$ and $\bm{R}_t$ in the Theoretical Part. Further we compute the log likelihood associated with the parameters and the values of $\bm{Q}_t$ and $\bm{R}_t$, using the established function $L_{C}(\theta,\Phi)$ as deduced in the Theoretical Part.

***Estimate_DCC*** estimates the DCC (CCC) model by first finding maximum likelihood estimates of our parameters, if we are in the DCC model, which are obtained by optimizing the negative log likelihood. The used optimizer, ***solnp***, is available through the **Rsolnp** R package, we set initial starting values, essentially we set starting value for $a$ and $b$. After convergence of a solution the final parameters are then feeded to the ***DCCFilter*** to obtain the final filtered values of $\bm{Q}_t$, $\bm{R}_t$ and the log likelihood value. We also compute the Bayesian Information Criterion.

## Empirical Part

Using the updated dataset of the S&P500 and DOW returns spanning from 2007-01-03 to 2019-01-01 we estimate the DCC and CCC model using the functions written in code 'Code_Flow_38.R' and described in Section \@ref(computational-part-1).

Table \@ref(tab:GSPCDJItab) show the five first observations.
```{r GSPCDJItab, echo=FALSE}
GSPC_DJI[1:5,1:2] %>% 
  knitr::kable(caption = 'The first 5 observations of the returns of SP500 and DOW.',
               booktabs=TRUE,
               align = c('r','r'),
               row.names = F,
               linesep = "") %>% 
   kableExtra::kable_styling(bootstrap_options = "striped",
                             latex_options = c("striped", "hold_position"),
                             full_width = F)
```

Figure \@ref(fig:GSPCfig) show the evolution of the S&P500 time series.
```{r GSPCfig, echo = FALSE, fig.align='center', fig.cap='SP500 returns from 2007-01-03 to 2019-01-01.', fig.pos='htbp!', fig.width=18, fig.asp=0.35}
# Plot VIX.
plot(y    = GSPC_DJI$GSPC,
     x    = index(head(GSPC$GSPC.Adjusted,-1)),
     type = 'l',
     xaxs = "i",
     yaxs = "i",
     xlab = 'Date',
     ylab = 'Return',
     main = 'SP500 Return Data')
```

Figure \@ref(fig:DJIfig) show the evolution of the DOW time series.
```{r DJIfig, echo = FALSE, fig.align='center', fig.cap='DOW returns from 2007-01-03 to 2019-01-01.', fig.pos='htbp!', fig.width=18, fig.asp=0.35}
# Plot VIX.
plot(y    = GSPC_DJI$DJI,
     x    = index(head(GSPC$GSPC.Adjusted,-1)),
     type = 'l',
     xaxs = "i",
     yaxs = "i",
     xlab = 'Date',
     ylab = 'Return',
     main = 'DOW Return Data')
```

### DCC

To obtain the DCC model for our data, we feed the relevant quantities to the relevant functions. The maximum likelihood estimates of our parameters are
$$\hat{\omega}=\begin{pmatrix}0.025\\0.023\end{pmatrix},\quad \hat{\alpha}=\begin{pmatrix}0.125\\0.13085261\end{pmatrix},\quad \hat{\beta}=\begin{pmatrix}0.857 \\ 0.851\end{pmatrix},\quad\hat{a}=0.070\quad\text{and}\quad \hat{b}=0.902.$$

We obtain a log likelihood of $-3688.162$ and a BIC of $7440.425$. Since the correlation matrix $\bm{R}$ is time-varying we will not post the results here. One can check the values throughout time using 'Code_Flow_38.R'.

### CCC

To obtain the CCC model for our data, we feed the relevant quantities to the relevant functions setting the boolean parameter 'CCC' to TRUE. Which evaluates our filter using $a=b=0$ as argued earlier. The maximum likelihood estimates of our parameters are exactly the same as for the DCC, since they are the outcome of the same two GARCH(1,1) processes.

The constant correlation matrix $\bm{R}$ is given as
$$\bm{R}=\begin{pmatrix}1.0000000 & 0.9671498\\ 0.9671498 & 1.0000000\end{pmatrix}.$$

Which show a very high correlation. We obtain a log likelihood of $-3894.175$ and a BIC of $7852.45$. Compared to the DCC model we observe a slightly worse BIC that yields the logic that allowing time-dependency in the correlation matrix yields better performance.

Continues on next page.

\newpage

### MVP

To obtain the weights associated with the Minimum Variance Portfolio we consider the risk-averse investors problem at day $t$
$$\min_{\omega_{t\mid t+h}} \omega_{t\mid t+h}'\bm{\Sigma}_{t\mid t+h}\omega_{t\mid t+h},\quad\quad \text{s.t.}\quad \omega_{t\mid t+h}'\ell=1.$$

The optimal solution can be deduced to 
\begin{equation}
\omega_{t\mid t+h}'^*=\frac{\bm{\Sigma}_{t\mid t+h}^{-1}\ell}{\ell'\bm{\Sigma}_{t\mid t+h}^{-1}\ell}.  (\#eq:mvp)
\end{equation}

Thus we use the above Equation \@ref(eq:mvp) to obtain the weights for the S&P500 Index and DOW throughout time. Figure \@ref(fig:weight-dcc-fig) shows the Minimum Variance Portfolio weights using the estimates obtain from the DCC model. Functions and documentation can be found in 'Code_Flow_38.R'.

```{r weight-dcc-fig, echo=FALSE, fig.align='center', fig.cap='Minimum Variance Portfolio Weights using DCC Model. Black line represents SP500 weights, red line represents DOW weights.', fig.pos='htbp!', fig.width=18, fig.asp=0.35}
plot(y    = weight_DCC[1,1,],
     x    = index(head(GSPC$GSPC.Adjusted,-1)),
     type ='l',
     ylim =c(-5,5),
     xaxs = "i",
     yaxs = "i",
     xlab = 'Date',
     ylab = 'Weight',
     main = 'Portfolio Weights for S&P500 and DOW using DCC'
     )

lines(y    = weight_DCC[1,2,],
      x    = index(head(GSPC$GSPC.Adjusted,-1)),
      type ='l',
      col  = 'red',
      lty = 'dashed')
```

One may notice the relatively high shorting positions. Figure \@ref(fig:weight-ccc-fig) shows the Minimum Variance Portfolio weights using the estimates obtain from the CCC model.

```{r weight-ccc-fig, echo=FALSE, fig.align='center', fig.cap='Minimum Variance Portfolio Weights using CCC Model. Black line represents SP500 weights, red line represents DOW weights.', fig.pos='htbp!', fig.width=18, fig.asp=0.35}
plot(y    = weight_CCC[1,1,],
     x    = index(head(GSPC$GSPC.Adjusted,-1)),
     type ='l',
     ylim =c(-5,5),
     xaxs = "i",
     yaxs = "i",
     xlab = 'Date',
     ylab = 'Weight',
     main = 'Portfolio Weights for S&P500 and DOW using CCC'
     )

lines(y    = weight_CCC[1,2,],
      x    = index(head(GSPC$GSPC.Adjusted,-1)),
      type = 'l',
      col  = 'red',
      lty = 'dashed')
```

Using the CCC model we observe somewhat more stable positions with diminished shorting positions. This makes sense due to the constant correlation matrix. That doesn't allow the returns to be more or less correlated through time.

### CoVaR

We want to compute the Conditional Value at Risk, which is the value at risk (VaR) of a financial system conditional on institutions being under distress. It is defined as the $\alpha$-quantile of the conditional distribution
$$Y_{1t}\mid Y_{2t}\leq \text{VaR}_{Y_{2t}}(\alpha).$$

And can also be defined as
$$\text{CoVaR}_{Y_{1t}\mid Y_{2t}}(\alpha,\alpha)=F^{-1}_{Y_{1t}\mid Y_{2t}\leq \text{VaR}_{Y_{2t}}(\alpha)}(\alpha).$$

We can find it as the solution to the equality below
$$F_{Y_{1t},Y_{2t}}(\text{CoVaR}_{Y_{1t}\mid Y_{2t}}(\alpha,\alpha),\text{VaR}_{Y_{2t}}(\alpha))=\alpha^2.$$

This is obtained using the *uniroot* function from the **stats** package in R. Figure \@ref(fig:covar-dcc-fig) shows the CoVaR using the estimates obtain from the DCC model. Functions and documentation can be found in 'Code_Flow_38.R'.

```{r covar-dcc-fig, echo=FALSE, fig.align='center', fig.cap='CoVaR using DCC Model. Black line represents CoVaR with significance level 0.01, red line represents CoVaR with signifiance level 0.05.', fig.pos='htbp!', fig.width=18, fig.asp=0.35}
plot(y = DCC_CoVaR_1,
     x    = index(head(GSPC$GSPC.Adjusted,-1)),
     type = 'l',
     xaxs = "i",
     yaxs = "i",
     xlab = 'Date',
     ylab = 'CoVaR',
     main = 'CoVaR of DCC Model')
lines(y = DCC_CoVaR_5,
      x    = index(head(GSPC$GSPC.Adjusted,-1)),
      type = 'l',
     xaxs = "i",
     yaxs = "i",
     col = 'red',
     lty = 'dashed')
```

One may notice the Financial Crisis, given the very big drop in CoVaR in 2008. Figure \@ref(fig:covar-ccc-fig) shows the CoVaR using the estimates obtain from the CCC model.

```{r covar-ccc-fig, echo=FALSE, fig.align='center', fig.cap='CoVaR using CCC Model. Black line represents CoVaR with significance level 0.01, red line represents CoVaR with signifiance level 0.05.', fig.pos='htbp!', fig.width=18, fig.asp=0.35}
plot(y= CCC_CoVaR_1,
     x    = index(head(GSPC$GSPC.Adjusted,-1)),
     type = 'l',
     xaxs = "i",
     yaxs = "i",
     xlab = 'Date',
     ylab = 'CoVaR',
     main = 'CoVaR of CCC Model')
lines(y = CCC_CoVaR_5,
      x    = index(head(GSPC$GSPC.Adjusted,-1)),
      type = 'l',
     xaxs = "i",
     yaxs = "i",
     col = 'red',
     lty = 'dashed')
```

The CoVaR of the CCC model is indeed very similar to DCC model and it's hard to tell where they differ. Thus we have now fully compared the DCC and CCC models.

\newpage